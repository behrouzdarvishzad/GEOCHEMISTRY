{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3b49d5c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook shows a clean ETL workflow for real geochemical data. Raw assay and mass‑spec files are messy, so the goal is to turn them into consistent, analysis‑ready tables using Python and pandas. The result is a reproducible pipeline suitable for geoscience analytics, PCA, clustering, and machine‑learning workflows.\n",
    "\n",
    "## ETL : Extract > Transform > Load:\n",
    "\n",
    "**Extract:** read raw files (CSV/Excel/SQL) into pandas\n",
    "\n",
    "**Transform:** clean columns, types, missing values, joins, filters\n",
    "\n",
    "**Load:** save clean tables (CSV/Parquet/SQL) for analysis or ML\n",
    "\n",
    "**`EXTRACT`** The following function provides a unified, flexible way to load raw geochemical and mass‑spec files, automatically selecting the correct parser for each format and preparing the data for the transformation steps that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19c9b8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def extract_file(path, *, delimiter=None, encoding=None, **read_kwargs):\n",
    "    \"\"\"\n",
    "    Load a tabular file into a DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str | Path\n",
    "        File path.\n",
    "    delimiter : str | None\n",
    "        Optional delimiter for text files (.txt, .tsv). If None, a sensible default is used.\n",
    "    encoding : str | None\n",
    "        Optional file encoding (e.g., 'utf-8', 'latin1', 'utf-16').\n",
    "    **read_kwargs :\n",
    "        Extra keyword args passed to pandas readers (e.g., dtype=..., na_values=..., engine=...).\n",
    "    \"\"\"\n",
    "    p = Path(str(path).strip())  # strip whitespace\n",
    "    ext = p.suffix.lower()\n",
    "\n",
    "    try:\n",
    "        if ext == \".csv\":\n",
    "            return pd.read_csv(p, encoding=encoding, **read_kwargs)\n",
    "\n",
    "        elif ext in (\".xlsx\", \".xlsm\", \".xltx\", \".xltm\"):\n",
    "            # Modern Excel formats → openpyxl is typically required\n",
    "            return pd.read_excel(p, engine=read_kwargs.pop(\"engine\", \"openpyxl\"),\n",
    "                                 dtype=read_kwargs.pop(\"dtype\", None),\n",
    "                                 **read_kwargs)\n",
    "\n",
    "        elif ext == \".xls\":\n",
    "            # Legacy Excel → xlrd may be required (and may not be installed)\n",
    "            return pd.read_excel(p, engine=read_kwargs.pop(\"engine\", \"xlrd\"),\n",
    "                                 dtype=read_kwargs.pop(\"dtype\", None),\n",
    "                                 **read_kwargs)\n",
    "\n",
    "        elif ext == \".tsv\":\n",
    "            return pd.read_csv(p, sep=\"\\t\" if delimiter is None else delimiter,\n",
    "                               encoding=encoding, **read_kwargs)\n",
    "\n",
    "        elif ext == \".txt\":\n",
    "            # Default to tab if not specified; allow override via `delimiter`\n",
    "            sep = \"\\t\" if delimiter is None else delimiter\n",
    "            return pd.read_csv(p, sep=sep, encoding=encoding, **read_kwargs)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {ext} for path {p}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Add context; re-raise for upstream handling/logging\n",
    "        raise RuntimeError(f\"Failed to parse file '{p}' (ext='{ext}'). \"\n",
    "                           f\"Consider specifying delimiter/encoding/engine. \"\n",
    "                           f\"Original error: {type(e).__name__}: {e}\") from e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afab2f17",
   "metadata": {},
   "source": [
    "### File Extraction Utility\n",
    "This function provides a flexible and robust way to load tabular data from a variety of common geoscience and laboratory file formats. It supports CSV, multiple Excel formats, TSV, and general text files, while allowing optional control over delimiters, encodings, and additional pandas reader arguments. The goal is to offer a single, consistent entry point for reading raw assay, mass‑spectrometry, or metadata files regardless of their source or formatting differences.\n",
    "\n",
    "The function automatically detects the file extension, applies an appropriate pandas reader, and includes clear error messaging when parsing fails. This makes it suitable for ETL pipelines where input files may vary in structure or require custom parsing options.\n",
    "\n",
    "The following snippet is an example of how to use the function; it is for demonstration only and does not need to be executed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76097ba8-fe0a-4f6d-961b-666efc88375f",
   "metadata": {},
   "source": [
    "```python\n",
    "from pathlib import Path\n",
    "\n",
    "# Example usage (illustrative only)\n",
    "file_path = Path(\"data/raw/assays_raw.csv\")\n",
    "df = extract_file(file_path)\n",
    "df.head()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0b172f-759d-4bdf-9bd8-57d0a5e22ad8",
   "metadata": {},
   "source": [
    "**`TRANSFORM`** Provides reusable data‑cleaning utilities for ETL workflows. Includes column standardization, dtype fixes, missing‑value handling,\n",
    "and simple filtering logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "badd2ffe-268e-4225-9db3-9209d186b17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def transform_assays(df: pd.DataFrame) -> pd.DataFrame:\n",
    "  \n",
    "    df = df.copy()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 1. Standardize column names\n",
    "    # ---------------------------------------------------------\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(\" \", \"_\")\n",
    "        .str.replace(\"-\", \"_\")\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2. Remove fully empty rows/columns\n",
    "    # ---------------------------------------------------------\n",
    "    df = df.dropna(how=\"all\")\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 3. Convert numeric columns safely\n",
    "    # ---------------------------------------------------------\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"object\":\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"ignore\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 4. Handle missing values (simple example)\n",
    "    # ---------------------------------------------------------\n",
    "    df = df.fillna(value={\"sample_id\": \"UNKNOWN\"})\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 5. Optional filtering logic (example only)\n",
    "    # ---------------------------------------------------------\n",
    "    if \"au_ppm\" in df.columns:\n",
    "        df = df[df[\"au_ppm\"] >= 0]  # remove negative gold grades\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c5457-48be-41d5-bb5a-aa7a3c5afd39",
   "metadata": {},
   "source": [
    "The transform module demonstrates:\n",
    "- Column name normalization\n",
    "- Removing empty rows/columns\n",
    "- Safe numeric conversion\n",
    "- Missing‑value handling\n",
    "- Example domain logic (e.g., removing negative Au grades)\n",
    "- A single, clean transform_assays() function\n",
    "\n",
    "\n",
    "**Transform Module (ICP‑MS Friendly- multi-element datasets)**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "007d5d35-c57e-4358-b357-c514718e6858",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TRANSFORM MODULE\n",
    "----------------\n",
    "Provides reusable, well‑documented cleaning utilities for multi‑element\n",
    "geochemistry (ICP‑MS) datasets. Each step is intentionally simple,\n",
    "transparent, and easy to adapt for real workflows.\n",
    "\n",
    "Example usage (illustrative only — replace with your own DataFrame):\n",
    "\n",
    "    from transform import transform_geochem\n",
    "    df_clean = transform_geochem(df_raw)\n",
    "    df_clean.head()\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def transform_geochem(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply standard cleaning steps to a multi‑element geochemistry dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Raw input DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Cleaned and standardized DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Work on a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 1. Standardize column names\n",
    "    # ---------------------------------------------------------\n",
    "    # Geochem datasets often come with inconsistent naming:\n",
    "    # \"Sample ID\", \"sample-id\", \"SAMPLEID\", etc.\n",
    "    # This normalizes everything to snake_case.\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(\" \", \"_\")\n",
    "        .str.replace(\"-\", \"_\")\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2. Remove fully empty rows/columns\n",
    "    # ---------------------------------------------------------\n",
    "    # ICP-MS exports sometimes include blank rows at the bottom OR empty columns used as separators.\n",
    "    df = df.dropna(how=\"all\")\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 3. Convert numeric columns safely\n",
    "    # ---------------------------------------------------------\n",
    "    # Many ICP-MS labs export numbers as strings, sometimes with\n",
    "    # \"<0.01\" or \"BDL\" (below detection limit). We convert what we can.\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"object\":\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"ignore\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 4. Handle detection limits (e.g., \"<0.01\")\n",
    "    # ---------------------------------------------------------\n",
    "    # Replace strings like \"<0.01\" with half the detection limit (0.005).\n",
    "    # This is a common geochem practice for statistical work.\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"object\":\n",
    "            mask = df[col].astype(str).str.startswith(\"<\")\n",
    "            if mask.any():\n",
    "                # Extract numeric part after \"<\"\n",
    "                dl = (\n",
    "                    df.loc[mask, col]\n",
    "                    .str.replace(\"<\", \"\", regex=False)\n",
    "                    .astype(float)\n",
    "                )\n",
    "                df.loc[mask, col] = dl / 2  # DL/2 substitution\n",
    "\n",
    "    # After replacing \"<DL\" values, try numeric conversion again\n",
    "    df = df.apply(pd.to_numeric, errors=\"ignore\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 5. Replace negative concentrations\n",
    "    # ---------------------------------------------------------\n",
    "    # Negative values appear when ICP-MS subtracts blank drift.\n",
    "    # They are not physically meaningful, so we set them to zero.\n",
    "    numeric_cols = df.select_dtypes(include=\"number\").columns\n",
    "    for col in numeric_cols:\n",
    "        df[col] = df[col].clip(lower=0)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 6. Handle missing sample IDs\n",
    "    # ---------------------------------------------------------\n",
    "    if \"sample_id\" in df.columns:\n",
    "        df[\"sample_id\"] = df[\"sample_id\"].fillna(\"UNKNOWN\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 7. Outlier removal using Median Absolute Deviation (MAD)\n",
    "    # ---------------------------------------------------------\n",
    "    # This is optional but useful for ICP-MS datasets with spikes.\n",
    "    def remove_outliers_mad(series: pd.Series, threshold: float = 5.0):\n",
    "        if not np.issubdtype(series.dtype, np.number):\n",
    "            return series\n",
    "        median = series.median()\n",
    "        mad = np.median(np.abs(series - median))\n",
    "        if mad == 0:\n",
    "            return series\n",
    "        z = 0.6745 * (series - median) / mad\n",
    "        return series.where(np.abs(z) < threshold)\n",
    "\n",
    "    df[numeric_cols] = df[numeric_cols].apply(remove_outliers_mad)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 8. Optional compositional transforms (CLR)\n",
    "    # ---------------------------------------------------------\n",
    "    # CLR is useful for multivariate geochem analysis.\n",
    "    # We apply it only to numeric columns and only if all values > 0.\n",
    "    def clr_transform(df_numeric: pd.DataFrame) -> pd.DataFrame:\n",
    "        # CLR requires strictly positive values\n",
    "        if (df_numeric <= 0).any().any():\n",
    "            return df_numeric\n",
    "        geometric_mean = df_numeric.apply(lambda row: np.exp(np.mean(np.log(row))), axis=1)\n",
    "        return df_numeric.div(geometric_mean, axis=0).apply(np.log)\n",
    "\n",
    "    # Example: store CLR-transformed data in a separate block\n",
    "    # (not replacing the original)\n",
    "    df_clr = clr_transform(df[numeric_cols])\n",
    "    df_clr.columns = [f\"{c}_clr\" for c in df_clr.columns]\n",
    "\n",
    "    # Attach CLR columns to the dataset\n",
    "    df = pd.concat([df, df_clr], axis=1)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e21709-3fa9-41ce-91e9-4c2663aacf23",
   "metadata": {},
   "source": [
    "##### Transform Stage\n",
    "The Transform step applies a series of cleaning and preprocessing operations to prepare raw ICP‑MS geochemistry data for analysis. The goal is to make the dataset consistent, numeric, and statistically usable while preserving geochemical meaning.\n",
    "\n",
    "This stage includes:\n",
    "\n",
    "- Column name standardization\n",
    "- Removal of empty rows/columns\n",
    "- Safe numeric conversion\n",
    "- Detection‑limit handling (<0.01 → 0.005)\n",
    "- Negative value correction\n",
    "- Missing value handling\n",
    "- Outlier removal using MAD\n",
    "- Optional CLR compositional transformation\n",
    "\n",
    "These steps are intentionally simple, transparent, and easy to adapt for real geochemistry workflows.\n",
    "\n",
    "1. Standardize Column Names\n",
    "Geochemistry datasets often contain inconsistent naming conventions such as \"Sample ID\", \"sample-id\", or \"SAMPLEID\".\n",
    "We convert everything to snake_case:\n",
    "\n",
    "- lowercase\n",
    "- spaces  \n",
    "- hyphens \n",
    "\n",
    "This makes downstream processing predictable.\n",
    "\n",
    "2. Remove Empty Rows and Columns\n",
    "ICP‑MS exports sometimes include blank rows at the bottom or empty separator columns. These are removed to keep the dataset clean.\n",
    "\n",
    "3. Convert Numeric Columns Safely\n",
    "Many labs export numbers as strings, sometimes mixed with text like \"BDL\" or \"<0.01\". We attempt numeric conversion without breaking non‑numeric columns.\n",
    "\n",
    "4. Handle Detection Limits (<DL)\n",
    "Values like \"<0.01\" are common in ICP‑MS data. We replace them with half the detection limit:\n",
    "\n",
    "\"<0.01\" → 0.005\n",
    "\n",
    "This is a widely used geochemical practice for statistical analysis.\n",
    "\n",
    "5. Replace Negative Concentrations\n",
    "Negative values occur when blank drift correction overshoots. They are not physically meaningful, so we set them to zero.\n",
    "\n",
    "6. Handle Missing Sample IDs\n",
    "If a sample_id column exists, missing values are replaced with \"UNKNOWN\".\n",
    "\n",
    "7. Remove Outliers Using MAD\n",
    "The Median Absolute Deviation (MAD) method removes extreme spikes without assuming a normal distribution. This is safer for geochemical data than z‑scores.\n",
    "\n",
    "8. Optional CLR Transformation\n",
    "The Centred Log‑Ratio (CLR) transform is useful for multivariate geochemical analysis. We apply CLR only when:\n",
    "- all values are positive\n",
    "- the column is numeric\n",
    "\n",
    "CLR‑transformed columns are added with a _clr suffix.\n",
    "\n",
    "\n",
    "**`Load Stage`**\n",
    "The Load step finalizes the ETL workflow by saving the cleaned dataset into a consistent, analysis‑ready format.\n",
    "This stage ensures that:\n",
    "\n",
    "- The processed data is reproducible\n",
    "- Downstream notebooks can load it easily\n",
    "- The file structure remains clean and predictable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7502c914-e099-4b97-b3bd-550d465ee27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LOAD MODULE\n",
    "-----------\n",
    "Handles saving cleaned datasets to disk in a consistent, safe, and\n",
    "reproducible way.\n",
    "\n",
    "Example usage (illustrative only — replace with your own DataFrame):\n",
    "\n",
    "    from load import load_data\n",
    "    load_data(df_clean, \"data/processed/assays_clean.csv\")\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_data(df: pd.DataFrame, output_path: str | Path) -> None:\n",
    "    \"\"\"\n",
    "    Save a cleaned DataFrame to disk in CSV format.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Cleaned dataset to save.\n",
    "    output_path : str or Path\n",
    "        Destination file path (e.g., 'data/processed/output.csv').\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    RuntimeError\n",
    "        If saving fails for any reason.\n",
    "    \"\"\"\n",
    "\n",
    "    output_path = Path(output_path)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 1. Ensure the output directory exists\n",
    "    # ---------------------------------------------------------\n",
    "    if not output_path.parent.exists():\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2. Save the DataFrame safely\n",
    "    # ---------------------------------------------------------\n",
    "    try:\n",
    "        df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            f\"Failed to save file to '{output_path}'. \"\n",
    "            f\"Original error: {e}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5a3df8-5549-45af-807f-a40f26cece64",
   "metadata": {},
   "source": [
    "The load_data() function completes the ETL pipeline by:\n",
    "- Creating output directories automatically\n",
    "- Saving cleaned datasets in a consistent format\n",
    "- Ensuring reproducibility for downstream analysis\n",
    "- Providing clear, user‑friendly error messages\n",
    "\n",
    "\n",
    "## Summary\n",
    "This project provides a small, modular ETL framework written in Python to demonstrate how geochemistry data can be processed cleanly and reproducibly. The code is organized into three simple modules: one for extracting raw files, one for transforming and cleaning the data, and one for saving the final output. The transform step includes geochemistry‑aware logic such as handling detection limits, fixing numeric values, removing outliers, and adding optional CLR features. Each module is intentionally lightweight, readable, and easy to adapt to other datasets. The goal is to give users a clear example of how to build a professional ETL workflow in Python without unnecessary complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e1b4a9-6ae1-4289-a31e-7df18b6b4507",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
